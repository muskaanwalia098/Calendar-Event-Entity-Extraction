lora:
  r: 16  # Conservative rank for stable training
  alpha: 32  # Scaled alpha accordingly (2x rank)
  dropout: 0.1  # Standard dropout for regularization
  target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - gate_proj  # Additional modules for better coverage
    - up_proj
    - down_proj
qlora:
  enabled: false  # Disable QLoRA for MPS - use native LoRA instead
  load_in_4bit: false
  bnb_4bit_use_double_quant: false
  bnb_4bit_quant_type: nf4
  bnb_4bit_compute_dtype: float32  # Use float32 for MPS compatibility
