seed: 42
paths:
  train: data/splits/train.jsonl
  eval: data/splits/eval.jsonl
  test: data/splits/test.jsonl
  outputs: models/
  best_outputs: simple_output/checkpoint-277/
model:
  base_id: HuggingFaceTB/SmolLM-360M
  max_length: 384
  max_new_tokens: 160
  bf16: false
  fp16: false  # Disable mixed precision for MPS stability
  device: auto  # Auto-detect best device
training:
  epochs: 3  # Reasonable epochs for assignment dataset (792 examples)
  per_device_train_batch_size: 4  # Good batch size for training
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2  # Effective batch size 8
  lr: 5.0e-4  # More conservative LR to avoid gradient issues
  weight_decay: 0.01
  lr_scheduler_type: cosine
  warmup_ratio: 0.1  # 10% warmup
  logging_steps: 25  # Log every 25 steps
  eval_steps: 100  # Evaluate every 100 steps
  save_steps: 100
  early_stopping_patience: 3
  dataloader_num_workers: 0  # Use main thread for MPS compatibility
  gradient_checkpointing: false  # Disable for MPS stability
  ddp: false
  adam_beta1: 0.9
  adam_beta2: 0.999
  use_mps_device: false  # Disable for compatibility
inference:
  temperature: 0.0
  top_p: 1.0
  do_sample: false
