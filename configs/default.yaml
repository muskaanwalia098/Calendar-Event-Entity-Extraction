seed: 42
paths:
  train: data/splits/train.jsonl
  eval: data/splits/eval.jsonl
  test: data/splits/test.jsonl
  outputs: models/
  best_outputs: models/best/
model:
  base_id: HuggingFaceTB/SmolLM-360M
  max_length: 384
  max_new_tokens: 160
  bf16: false
  fp16: true
training:
  epochs: 8
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 2
  lr: 2.0e-5
  weight_decay: 0.01
  lr_scheduler_type: cosine
  warmup_ratio: 0.10
  logging_steps: 20
  eval_steps: 200
  save_steps: 200
  early_stopping_patience: 5
  dataloader_num_workers: 2
  gradient_checkpointing: true
  ddp: false
  adam_beta1: 0.9
  adam_beta2: 0.999
inference:
  temperature: 0.0
  top_p: 1.0
  do_sample: false
